{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f972f82d",
   "metadata": {},
   "source": [
    "### Basic working of Google Palm LLM in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34aa70b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T03:26:44.355626Z",
     "start_time": "2023-11-11T03:26:42.902388Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import google-generativeai python package. Please install it with `pip install google-generativeai`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/share/virtualenvs/AutoGluon-Course-PTu5Ivpl/lib/python3.10/site-packages/langchain/llms/google_palm.py:113\u001B[0m, in \u001B[0;36mGooglePalm.validate_environment\u001B[0;34m(cls, values)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgenerativeai\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mgenai\u001B[39;00m\n\u001B[1;32m    115\u001B[0m     genai\u001B[38;5;241m.\u001B[39mconfigure(api_key\u001B[38;5;241m=\u001B[39mgoogle_api_key)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'google.generativeai'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GooglePalm\n\u001B[1;32m      3\u001B[0m api_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mapi_key\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 5\u001B[0m llm \u001B[38;5;241m=\u001B[39m \u001B[43mGooglePalm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgoogle_api_key\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mapi_key\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/AutoGluon-Course-PTu5Ivpl/lib/python3.10/site-packages/langchain/load/serializable.py:97\u001B[0m, in \u001B[0;36mSerializable.__init__\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 97\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lc_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/AutoGluon-Course-PTu5Ivpl/lib/python3.10/site-packages/pydantic/main.py:339\u001B[0m, in \u001B[0;36mpydantic.main.BaseModel.__init__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/AutoGluon-Course-PTu5Ivpl/lib/python3.10/site-packages/pydantic/main.py:1102\u001B[0m, in \u001B[0;36mpydantic.main.validate_model\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/AutoGluon-Course-PTu5Ivpl/lib/python3.10/site-packages/langchain/llms/google_palm.py:117\u001B[0m, in \u001B[0;36mGooglePalm.validate_environment\u001B[0;34m(cls, values)\u001B[0m\n\u001B[1;32m    115\u001B[0m     genai\u001B[38;5;241m.\u001B[39mconfigure(api_key\u001B[38;5;241m=\u001B[39mgoogle_api_key)\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m    118\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not import google-generativeai python package. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    119\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install it with `pip install google-generativeai`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    120\u001B[0m     )\n\u001B[1;32m    122\u001B[0m values[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclient\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m genai\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m values[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m values[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "\u001B[0;31mImportError\u001B[0m: Could not import google-generativeai python package. Please install it with `pip install google-generativeai`."
     ]
    }
   ],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "\n",
    "api_key = 'api_key'\n",
    "\n",
    "llm = GooglePalm(google_api_key=api_key, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227816a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T03:26:44.358579Z",
     "start_time": "2023-11-11T03:26:44.358118Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765695b5",
   "metadata": {},
   "source": [
    "Load data from Codebasics FAQ csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62e35c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-11T03:26:44.359529Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='2016-2021-imi-results-long-view.csv', source_column=\"prompt\")\n",
    "\n",
    "# Store the loaded data in the 'data' variable\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd45e51",
   "metadata": {},
   "source": [
    "### Hugging Face Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4de8c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-11T03:26:44.361325Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# Initialize instructor embeddings using the Hugging Face model\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-large\")\n",
    "\n",
    "e = instructor_embeddings.embed_query(\"Predict # of death based on hospital rating ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762eeac",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-11T03:26:44.363246Z"
    }
   },
   "outputs": [],
   "source": [
    "len(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fab6ce",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-11T03:26:44.365034Z"
    }
   },
   "outputs": [],
   "source": [
    "e[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571c0d2",
   "metadata": {},
   "source": [
    "As you can see above, embedding for a sentance \"What is your refund policy\" is a list of size 768. Looking at the numbers in this list, doesn't give any intuitive understanding of what it is but just assume that these numbers are capturing the meaning of \"What is your refund policy\". If you are curious to know about embeddings, go to youtube and search \"codebasics word embeddings\" and you will find bunch of videos with simple, intuitive explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc80a28a",
   "metadata": {},
   "source": [
    "### Vector store using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c706da",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-11T03:26:44.367042Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Create a FAISS instance for vector database from 'data'\n",
    "vectordb = FAISS.from_documents(documents=data,\n",
    "                                 embedding=instructor_embeddings)\n",
    "\n",
    "# Create a retriever for querying the vector database\n",
    "retriever = vectordb.as_retriever(score_threshold = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6b257",
   "metadata": {},
   "source": [
    "As you can see above, the retriever that was created using FAISS and hugging face embedding is now capable of pulling relavant documents from our original CSV file knowledge store. This is very powerful and it will help us further in our project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bee857",
   "metadata": {},
   "source": [
    "##### Embeddings can be created using GooglePalm too. Also for vector database you can use chromadb as well as shown below. During our experimentation, we found hugging face embeddings and FAISS to be more appropriate for our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d079d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-11T03:26:44.368752Z"
    }
   },
   "outputs": [],
   "source": [
    "# google_palm_embeddings = GooglePalmEmbeddings(google_api_key=api_key)\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "# vectordb = Chroma.from_documents(data,\n",
    "#                            embedding=google_palm_embeddings,\n",
    "#                            persist_directory='./chromadb')\n",
    "# vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3d927",
   "metadata": {},
   "source": [
    "### Create RetrievalQA chain along with prompt template ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4842c8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-11T03:26:44.370297Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
    "In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
    "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                            chain_type=\"stuff\",\n",
    "                            retriever=retriever,\n",
    "                            input_key=\"query\",\n",
    "                            return_source_documents=True,\n",
    "                            chain_type_kwargs=chain_type_kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a4cf8",
   "metadata": {},
   "source": [
    "### We are all set ðŸ‘ðŸ¼ Let's ask some questions now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90166e8d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-11T03:26:44.371677Z"
    }
   },
   "outputs": [],
   "source": [
    "chain('What can you tell about number of death based on the risk adjusted mortality rate?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
